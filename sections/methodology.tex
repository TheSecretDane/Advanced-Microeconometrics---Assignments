\section{Methodology} \label{sec:methodology}

We start by taking logs of \eqref{eq:CD} to linearize it,
\begin{align*}
    \ln Y_{it} &= \ln A_{it} + \beta_K \ln K_{it} + \beta_L \ln L_{it} \qquad \Rightarrow
    y_{it} = \beta_K k_{it} + \beta_L l_{it} + \varepsilon_{it}, \quad \varepsilon_{it} = \ln A_{it}
\end{align*}
where $y_{it} = \ln Y_{it}$ and so on. Suppose that $\ln A_{it}$ can be decomposed into a time-invariant firm-specific effect, $c_i$ and add an idiosyncratic error, $u_{it}$, such that the composite error is $v_{it} = c_i + u_{it}$. Our model is then:
\begin{align*}
    y_{it} &= \beta_K k_{it} + \beta_L l_{it}+c_i + u_{it}
\end{align*}
We write the model on compact form by stacking the regressors in the vector $\bm{x}_{it} = (k_{it}, l_{it})$ and the parameters in the vector $\bm{\beta} = (\beta_K, \beta_L)'$.
\begin{align}
    y_{it} = c_i + \bm{x}_{it} \bm{\beta} + u_{it}, \quad t=1,2,\ldots,8, \quad i=1,2,\ldots,441
    \label{equationline}
\end{align}

\subsubsection*{Pooled OLS}
The pooled OLS (POLS) estimator performs OLS on the entire panel, treating each $(i,t)$ observation as i.i.d. In POLS, identification of $\pmb{\beta}$ requires  $\mathbb{E}[\pmb{x}_{it}'v_{it}]=\mathbb{E}[\pmb{x}_{it}'\alpha_{i}]+\mathbb{E}[\pmb{x}_{it}'u_{it}]=0$.
%In the context of the data, both the time-varying and time-invariant part of TFP must then be assumed uncorrelated with both the log of capital and employment.
An example of $\mathbb{E}[\pmb{x}_{it}'v_{it}]\neq0$ is that if there is some firm-specific effect on the log of sales that correlates with the log of capital or employment, i.e. $\mathbb{E}[\pmb{x}_{it}'c_{i}]\neq0$. Since we find this likely, we consider the class of Fixed Effects Methods to alleviate the omitted variable problem in POLS.

\subsubsection*{Fixed Effects}
The Fixed Effects (FE) estimator doesn't suffer from same identification issues as the POLS estimator. We show this by performing a within-transformation on \eqref{equationline}. For each firm $i$, we subtract the average (over $T$) dependent variable  on the LHS and the average (over $T$) regressors on the RHS. In this way, the time-invariant TFP $\alpha_i$ cancels out. Define the demeaned variable as $ \ddot{y}_{it}, \pmb{\ddot{x}}_{it}, \ddot{u}_{it}$.
\begin{align*}
    y_{it}-\bar{y}_{i} &=(\pmb{x}_{it}-\bar{\pmb{x}}_{i}) \pmb{\beta}+(\alpha_i-\alpha_i)+(u_{it} - \bar{u}_{i}) \\
    \ddot{y}_{it} &=  \ddot{\pmb{x}}_{it} \pmb{\beta}+\ddot{u}_{it}
\end{align*}
Identification of $\pmb{\beta}$ is considered. For simplicity, we stack the data over the time-dimension such that $\ddot{\pmb{y}}_{i}$ and $\ddot{\pmb{u}}_{i}$ are $T \times 1$ vectors and $\pmb{\ddot{X}}$ is a $T\times K$ matrix. Using the model equation, premultiplying $\pmb{\ddot{X}}_{i}$, taking expectations and rearranging, we get the following.
$$ \Rightarrow  \pmb{\beta} = (\mathbb{E}[\pmb{\ddot{X}}_{i}'\pmb{\ddot{X}}_{i}])^{-1} \left(  \mathbb{E}[\pmb{\ddot{X}}_{i}'\pmb{\ddot{y}}_{i}]-\mathbb{E}[\pmb{\ddot{X}}_{i}' \pmb{\ddot{u}}_{i}]\right)$$
, i.e. we see that $\pmb{\beta}$ is identified as $\pmb{\beta} = (\mathbb{E}[\pmb{\ddot{X}}_{i}'\pmb{\ddot{X}}_{i}])^{-1} \mathbb{E}[\pmb{\ddot{X}}_{i}'\pmb{y}_{i}]$ if we assume the following:
\begin{enumerate}
    \item[\textbf{FE.1}] \underline{Strict exogenity} $\mathbb{E}[u_{it}\vert \pmb{x}_{i1},\pmb{x}_{i2}\dots,\pmb{x}_{iT},\alpha_i]=0\Rightarrow \mathbb{E}[\pmb{\ddot{X}}_{i}' \pmb{\ddot{u}}_{i}]=0$ \\
    The time-varying error term must be exogenous to both log of capital and employment, reflecting no dependence of production shocks on log of capital or employment, neither contemporaneous, leaded or lagged.
    \item[\textbf{FE.2}] \underline{ (full) rank condition}: $\text{rank}(\mathbb{E}[\pmb{\ddot{X}}_{i}'\pmb{\ddot{X}}_{i}])=K$ \\
    Log of capital and employment can't be linearly dependent.
\end{enumerate}

Estimation of $\pmb{\beta}$ under FE.1 and FE.2 using the analogy principle gives the following.
\begin{equation}
    \pmb{\hat{\beta}}_{FE}= \left( \frac{1}{N} \sum_{i=1}^N {\pmb{\ddot{X'}}_{i} \pmb{\ddot{X}}_{i} } \right) ^{-1}\frac{1}{N} \sum_{i=1}^N {\pmb{\ddot{X'}}_{i} \pmb{\ddot{y}}_{i} } \label{EstimationEq}
\end{equation}
, where we emphasize that the former expectation for the population is over the firms $i$ (and not $t$).

%% With $\pmb{\ddot{X}}'\pmb{\ddot{X}}$, we obtain a $(K \times K)$ matrix, which, with a $(K \times 1)$ vector from $\pmb{\ddot{X'}}_{i} \pmb{\ddot{y}}_{i}$ results in a $(K \times 1)$ vector $\pmb{\beta}$.

\subsubsection*{Consistency of Fixed Effects estimator}
We evaluate the consistency of the FE-estimator $\pmb{\hat{\beta}}_{FE}$ by Inserting $\pmb{\ddot{y}}_i$ in \eqref{EstimationEq}.
\begin{align}
    \Rightarrow \pmb{\hat{\beta}}_{FE} &= \left( \frac{1}{N} \sum_{i=1}^N {\pmb{\ddot{X'}}_{i} \pmb{\ddot{X}}_{i} } \right) ^{-1} \left(\frac{1}{N} \sum_{i=1}^N {\pmb{\ddot{X'}}_{i}}\pmb{\ddot{X}}_{i} \pmb{\beta}+\pmb{\ddot{X'}}_{i}\pmb{\ddot{u}}_i \right) \nonumber = \pmb{\beta} +\left( \frac{1}{N} \sum_{i=1}^N {\pmb{\ddot{X'}}_{i} \pmb{\ddot{X}}_{i} } \right) ^{-1} \left( \frac{1}{N} \sum_{i=1}^N {\pmb{\ddot{X}}_i' \pmb{\ddot{u}}_i} \right) \label{FE-estimator}
\end{align}
Under FE.1 and FE.2, a Law of Large Numbers (LLN) and Slutsky's theorem (using that the inverse of a matrix is continuous mapping) shows us that $\pmb{\hat{\beta}}_{FE}$ is consistent

%%we can then we apply a Law of Large Numbers (LLN) such that $p\text{-lim}\left(\frac{1}{N} \sum_{i=1}^N {\pmb{\ddot{X'}}_{i} \pmb{\ddot{X}}_{i} } \right)=\mathbb{E}[\pmb{\ddot{X'}}_{i} \pmb{\ddot{X}}_{i}]$ and $p\text{-lim}\left(\frac{1}{N} \sum_{i=1}^N {\pmb{\ddot{X'}}_{i} \pmb{\ddot{u}}_{i} } \right)=\mathbb{E}[\pmb{\ddot{X'}}_{i} \pmb{\ddot{u}}_{i}]=0$. Then we use Slutskys theorem: $p\text{-lim}\left( \left(\frac{1}{N} \sum_{i=1}^N {\pmb{\ddot{X'}}_{i} \pmb{\ddot{X}}_{i} } \right)^{-1}\right)=(\mathbb{E}[\pmb{\ddot{X'}}_{i} \pmb{\ddot{X}}_{i}])^{-1}$, which is allowed since the inverse matrix is defined, due to the rank assumption, and the inverse operator is a continuous function.
$$p\text{-lim}(\pmb{\hat{\beta}}_{FE})=\pmb{\beta} + \left( \mathbb{E}[\ddot{\pmb{X'}}_i \ddot{\pmb{X}}_i] \right)^{-1}\mathbb{E}[\ddot{\pmb{X'}}_i \ddot{\pmb{u}}_i]=\pmb{\beta} + \left( \mathbb{E}[\ddot{\pmb{X'}}_i \ddot{\pmb{X}}_i] \right)^{-1}*\pmb{0} =\pmb{\beta}$$

\subsubsection*{Asymptotic normality of Fixed Effects estimator}
Rearranging \eqref{EstimationEq} and using $\mathbb{E}[\sqrt{N}(\pmb{\hat{\beta}}_{FE}-\pmb{\beta})]=\pmb{0}$ under FE.1 and FE.2, we see that the sum of products of regressors and time-varying errors converge according to the Central Limit Theorem (CLT). The product rule then implies that the product of the two matrices converges (only) in distribution.
\begin{align*}
    \sqrt{N}(\pmb{\hat{\beta}}_{FE}-\pmb{\beta})&=  \left( \frac{1}{N} \sum_{i=1}^N {\pmb{\ddot{X'}}_{i} \pmb{\ddot{X}}_{i} } \right) ^{-1} \left( \frac{1}{\sqrt{N}} \sum_{i=1}^N {\pmb{\ddot{X}}_i' \pmb{\ddot{u}}_i} \right) 
    \text{N} \rightarrow_d \left(\pmb{0},
(\mathbb{E}[\pmb{\ddot{X'}}_{i} \pmb{\ddot{X}}_{i}])^{-1} \mathbb{E}[\pmb{\ddot{X}}_i' \pmb{\ddot{u}}_i\pmb{\ddot{u}}_i'\pmb{\ddot{X}}_i] (\mathbb{E}[\pmb{\ddot{X'}}_{i} \pmb{\ddot{X}}_{i}])^{-1} \right)
\end{align*}

%Earlier, we saw that FE.1 and FE.2 combined with LLN and Slutsky's theorem implied that asymptotically $\mathbb{E}[\sqrt{N}(\pmb{\hat{\beta}}_{FE}-\pmb{\beta})]=\pmb{0}$. This means that the sum of real-valued i.i.d. r.v.s converges accordingly to the Central Limit Theorem such that $\left( \frac{1}{\sqrt{N}} \sum_{i=1}^N {\pmb{\ddot{X}}_i' \pmb{\ddot{u}}_i} \right)\rightarrow _d \text{N}(\pmb{0},\mathbb{E}[\pmb{\ddot{X}}_i' \pmb{\ddot{u}}_i(\pmb{\ddot{X}}_i' \pmb{\ddot{u}}_i)'])=^d \text{N}(\pmb{0},\mathbb{E}[\pmb{\ddot{X}}_i' \pmb{\ddot{u}}_i\pmb{\ddot{u}}_i'\pmb{\ddot{X}}_i])$.

%%Since we have convergence in probability $p\text{-lim}\left( \left(\frac{1}{N} \sum_{i=1}^N {\pmb{\ddot{X'}}_{i} \pmb{\ddot{X}}_{i} } \right)^{-1}\right)=(\mathbb{E}[\pmb{\ddot{X'}}_{i} \pmb{\ddot{X}}_{i}])^{-1}$ for the first term and convergence in distribution for the second term, %%
%The product rule implies that the expression for $\sqrt{N}(\pmb{\hat{\beta}}_{FE}-\pmb{\beta})$ converges only in the distributional sense such that:
%$$\Rightarrow \sqrt{N}(\pmb{\hat{\beta}}_{FE}-\pmb{\beta}) \sim \text{N} \left(\pmb{0},
%(\mathbb{E}[\pmb{\ddot{X'}}_{i} \pmb{\ddot{X}}_{i}])^{-1} \mathbb{E}[\pmb{\ddot{X}}_i' \pmb{\ddot{u}}_i\pmb{\ddot{u}}_i'\pmb{\ddot{X}}_i] (\mathbb{E}[\pmb{\ddot{X'}}_{i} \pmb{\ddot{X}}_{i}])^{-1} \right)$$

\input{sections/meth/efficiency.tex}

\subsubsection*{Wald-test}
%(Rækkefølgen skal ændres på, da nogle antagelser bruges i de tidligere underafsnit)
We utilise the Wald-test to test for CRS, where the null-hypothesis is $H_0: \mathbf{R}\boldsymbol{\beta}=\mathbf{r}$. Under weak regularity conditions - consistency of the FE estimator, asymptotic normality and $\mathbf{R}$ has full rank, . These conditions assumes assumption \textbf{FE.1-2} are valid and we are using robust standard errors. The Wald statistic can be written as:
\begin{equation*}
    W:=(\mathbf{R}{\boldsymbol{\hat\beta}}-\mathbf{r})'
    [\mathbf{R}\widehat{\text{Avar}({\boldsymbol{\hat{\beta}}}_{FE})}\mathbf{R}']^{-1}
    (\mathbf{R} \boldsymbol{\hat{\beta}}-\mathbf{r})
\end{equation*}

When testing for CRS we test the linear hypothesis
\begin{equation*}
  H_0: \mathbf{R}\boldsymbol{\beta}=\mathbf{r} \Leftrightarrow
\begin{bmatrix} 1&1 \end{bmatrix} \begin{bmatrix} \beta_K \\ \beta_L \end{bmatrix} =1
\Leftrightarrow \beta_K+\beta_L=1  
\end{equation*}


Under the $H_0$ the Wald statistic follows a $\chi^2_Q$ distribution.

%begone thot
When testing for CRS we test $H_0:\beta_K+\beta_L=1$, where we have $Q=1$ degrees of freedom and  $\mathbf{R}$ is a $[1 \times K]$ matrix and $\textbf{r}=1 \times 1$ matrix, where $K$ is the rank $K=E[\textbf{x}'\textbf{x}]$  from assumption FE.2. Since we only test one linear hypothesis the Walt-statistic is simply the squared t-statistic (might be very overkill to state this mathematically, but in terms of our actual estimates:)
The null-hypothesis is given by  $H_0: \mathbf{R}\boldsymbol{\hat{\beta}}=\mathbf{r}\Leftrightarrow \begin{bmatrix}1&1 \end{bmatrix} \begin{bmatrix}\hat{\beta}_K\\ \hat{\beta}_L \end{bmatrix}=1$. Insert the estimated values for $\hat{\beta}$ and the Walt-statistic yields:
\begin{equation*}
    W_{robust}= (\begin{bmatrix}1&1 \end{bmatrix} 
    \begin{bmatrix} 0.154620 \\ 0.694226 \end{bmatrix}-1)[\begin{bmatrix}1&1 \end{bmatrix}
    \begin{bmatrix}
        0.001735 & -0.000727 \\ 
        -0.000727 & 0.0008968
    \end{bmatrix} \begin{bmatrix}1&1 \end{bmatrix}-1]\approx 19.402912
\end{equation*}
If the assumptions for the Wald-test holds (assumptions FE.1-2 for $W_{robust}$), then the squared t-statistic, $t^2$, is equivalent to the Wald-statistic. The framework for one linear restriction is a little different, but remember the general t-statistic looks like:

\begin{equation*}
    t=\frac{\hat{\beta}-\beta}{se(\hat{\beta})}
\end{equation*}
The framework for testing a linear restriction when using a t-test is $H_0:\textbf{R}\boldsymbol{\beta}=\hat{\textbf{r}}$, where we estimate $\hat{\textbf{r}}$ instead of stating what it must be. From the null-hypothesis from the Wald-test we simply state $\textbf{r}=1$, but now we try to estimate $\hat{\textbf{r}}$, and the simple t-test setup looks like the classical t-test framework $H_0: \hat{\beta}=\beta\Rightarrow$
\begin{align*}
    H_0:\hat{\textbf{r}}&=\textbf{r}\Leftrightarrow \hat{\textbf{r}}=1\Leftrightarrow  \\
     \textbf{R}\boldsymbol{\hat{\beta}}&=1\Leftrightarrow \hat{\beta}_K+\hat{\beta_L}=1
\end{align*}
We take $\beta\Rightarrow \textbf{r}$ as given because it is a value the statistician (we) declare $\textbf{r}=1$. Insert the framework into the t-statistic:
\begin{equation*}
    t=\frac{\hat{\beta}_K+\hat{\beta}_L-1}{se(\hat{\textbf{r}})}
    \Leftrightarrow \frac{0.154620+0.694226-1}{0.001178}\approx-4.404874
\end{equation*}
And t squared $t^2=(-4.404873)^2\approx19.402906\approx W_{robust}=19.402912$. Differences is caused by rounding errors \textit{run the code for exact results}.

(\textit{dette burde stå først}:) If we use normal standard errors, we require stronger assumptions where under assumption FE.3 the FE estimator is asymptotically efficient $\rightarrow$ we can use normal standard errors. If we relax assumption FE.3, or simply reject the null-hypothesis from a serial-correlation test, like a placebo test with an AR(1) model, (\textit{see section for serial correlation for this specific test}) we have to use robust standard errors. The only change in the Wald-statistic is the variance-covariance matrix $\text{Avar}(\hat{\beta}_{FE})$, where the robust variance-covariance matrix is given by the sandwich formula. \newline
\textit{As seen under the efficiency section. We need to differentiate between the normal and robust version of Avar($\beta$)}.



\subsubsection*{Serial correlation test}
In this report we utilise an autoregressive process with one lag (AR(1)) to test for serial correlation in the error term. We use an auxilliary regression:
\begin{equation*}
    \hat{\ddot{u}}_{it}=\rho \hat{\ddot{u}}_{it-1}+\varepsilon_{it}
\end{equation*}
If $\hat{\ddot{u}}_{it-1}$has a significant impact on $\hat{\ddot{u}}_{it}$ it indicates a problem that the errors are correlated across (some) time. This warrants at least using robust standard errors, but since the time-demeaned errors are serially correlated, we have to use robust standard errors for the t-test/Wald-test to be valid. If there is arbitrary serial correlation, the t-statistic is not valid and since $\ddot{u}_{it},\ddot{u}_{it-1}$ is correlated (\textit{tends to 0 as T grows, so asymptotic properties are still exactly the same as always...)}. So using time-demeaned errors, we test for serial correlation in these errors by making the test robust to arbitrary serial correlation. The t-statistic will be valid, and if it still shows significance, it does indicate a problem. If this is the case we have to use robust standard errors in our model when estimating $\beta_K,\beta_K$, to be able to conduct valid tests.

\subsubsection*{Strict exogeneity}

To test for strict exogeneity we test whether any (is it any, or...?) $\bm{x_{is}}$ is correlated with $\bm{u}_{it}$ for $s \neq t$. This is most easily done by testing the significance of parameter estimates on leaded explanatory variables, $\bm{x}_{it+1}$, that is estimate, 
\begin{align*}
    y_{it} = \bm{x}_{it}\beta + \bm{w}_{it+1} \bm{\delta} + c_i + u_{it}
\end{align*} 
using FE, where $\bm{w}_{it+1} \subseteq \bm{x}_{it+1}$. Under strict exogeneity $\bm{\delta} = 0$, which is done by a regular t-test. Similarly for the FD model following \cite[Chapter~10]{wooldridgeEconometricAnalysisCross2010}. 

\subsubsection*{Homoskedasticity}
No
